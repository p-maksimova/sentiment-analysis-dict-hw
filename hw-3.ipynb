{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e04d5af",
   "metadata": {},
   "source": [
    "### Задание 1: составить словари для классификации по тональности.\n",
    "\n",
    "1. Разбейте всю коллекцию отзывов на предложения. Лемматизируйте все слова.\n",
    "2. Обучите по коллекции предложений word2vec\n",
    "3. Приведите несколько удачных и неудачных примеров решения стандартных текстов для word2vec:\n",
    "    - тест на определение ближайших слов\n",
    "    - тест на аналогии (мужчина – король : женщина – королева)\n",
    "    - тест на определение лишнего слова.\n",
    "4. Постройте несколько визуализаций:\n",
    "    - TSNE для топ-100 (или топ-500) слов и найдите осмысленные кластеры слов\n",
    "    - задайте координаты для нового пространства следующим образом: одна ось описывает отношение \"плохо – хорошо\", вторая – \"медленно – быстро\" и найдите координаты названий банков в этих координатах. Более формально: берем вектор слова \"хорошо\", вычитаем из него вектор слова \"плохо\", получаем новый вектор, который описывает разницу между хорошими и плохими словами. Берем вектор слова \"сбербанк\" и умножаем его на этот новый вектор – получаем координату по первой оси. Аналогично – для второй оси. Две координаты уже можно нарисовать на плоскости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951ef023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\garti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\garti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import bz2\n",
    "import regex\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import ngrams\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec59826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201030it [00:41, 4884.33it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:\n",
    "    for row in tqdm(thefile):\n",
    "        resp = json.loads(row)\n",
    "        if not resp['rating_not_checked'] and (len(resp['text'].split()) > 0):\n",
    "            responses.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3266a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153499 entries, 0 to 153498\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   city                138325 non-null  object \n",
      " 1   rating_not_checked  153499 non-null  bool   \n",
      " 2   title               153499 non-null  object \n",
      " 3   num_comments        153499 non-null  int64  \n",
      " 4   bank_license        153498 non-null  object \n",
      " 5   author              153479 non-null  object \n",
      " 6   bank_name           153499 non-null  object \n",
      " 7   datetime            153499 non-null  object \n",
      " 8   text                153499 non-null  object \n",
      " 9   rating_grade        88658 non-null   float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(7)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074fa5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции предобработки текста\n",
    "\n",
    "# формируем словарь стоп-слов\n",
    "mystopwords = stopwords.words('russian') + [\n",
    "              'это', 'наш' , 'тыс', 'млн', 'млрд', 'также',  'т', 'д',\n",
    "              'который','прошлый','сей', 'свой', 'наш', 'мочь', 'такой', 'руб', 'г']\n",
    "\n",
    "# убераем пунктуацию    \n",
    "def words_only(text):\n",
    "    return \" \".join(re.compile(\"[А-Яа-яЁё]+\").findall(text))\n",
    "\n",
    "# приводим слова к нормальной форме\n",
    "def lemmatize(text, m=MorphAnalyzer()):\n",
    "    try:\n",
    "        return [m.parse(word)[0].normal_form for word in text.split(' ')]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# исключаем стоп-слова    \n",
    "def remove_stopwords(text, stopwords=mystopwords):\n",
    "    try:\n",
    "        return \" \".join([token for token in text if not token in stopwords])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# основная функция\n",
    "def preprocess(text):\n",
    "    text = nltk.tokenize.sent_tokenize(text) #делим на предложения\n",
    "    return np.array([remove_stopwords(lemmatize(words_only(sentence.lower()))) for sentence in text]) # сборка всех функций "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fd40c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 153499/153499 [1:56:23<00:00, 21.98it/s]\n"
     ]
    }
   ],
   "source": [
    "df['text_pp'] = df['text'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050909b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем DataFrame в файл CSV\n",
    "df.to_csv('df_pp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка DataFrame\n",
    "df = pd.read_csv('df_pp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb92fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2664065it [00:27, 97592.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.3 s\n",
      "Wall time: 27.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Инициаоищация модели\n",
    "# \n",
    "model = Word2Vec()\n",
    "model.build_vocab(tqdm(sentence for text in df['text_pp'] for sentence in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc516abb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a generator as corpus_iterable can't support 16 passes. Try a re-iterable sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:428\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload \u001b[38;5;241m=\u001b[39m call_on_class_only\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_corpus_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1506\u001b[0m, in \u001b[0;36mWord2Vec._check_corpus_sanity\u001b[1;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe corpus_iterable must be an iterable of lists of strings, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m corpus_iterable)\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(corpus_iterable, GeneratorType) \u001b[38;5;129;01mand\u001b[39;00m passes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1507\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a generator as corpus_iterable can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m passes. Try a re-iterable sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1510\u001b[0m     _, corpus_ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(corpus_file)\n",
      "\u001b[1;31mTypeError\u001b[0m: Using a generator as corpus_iterable can't support 16 passes. Try a re-iterable sequence."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(other_sentences)\n",
    "corpus = (sentence for text in df['text_pp'] for sentence in text)\n",
    "model = Word2Vec(corpus, workers=4, vector_size=300, min_count=1, window=7, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427b9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72234741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2cb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0df4f1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'добрый день'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e040fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acc441",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['text_pp'][:2].values\n",
    "np.hstack(temp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfe93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08e43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(df['text_pp'], workers=4, vector_size=300, min_count=1, window=7, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fee902",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# тест на определение ближайших слов  #most_similar('сотрудник')\n",
    "for i, w in enumerate(model.wv.index_to_key):\n",
    "    if i == 100:\n",
    "        break\n",
    "    print(i, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9065bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('телефон')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тест на аналогию\n",
    "model.wv.most_similar(positive=['отделение банк'], negative=['офис'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тест на определение лишнего слова\n",
    "model.wv.doesnt_match(['кредит', 'повод', 'росспотребнадзор'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e866e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a5d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cbd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7722b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fca73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd4437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754e815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af0698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310deeed",
   "metadata": {},
   "source": [
    "### Часть 2. Распространение метки\n",
    "Определите 5-8 позитивных слов (например, “быстрый”, “удобный”) и 5-8 негативных слов (например,“очередь”, “медленно”). Эти слова будут основной будущего оценочного словаря. Пусть позитивному классу соответствует метка 1, негативному – -1. Пометьте выбранные слова в лексическом графе соответствующими метками. Запустите любой известный вам метод распространения метки (Label Propogation) в лексическом графе. На выходе метода распространения ошибки должны быть новые слова, помеченные метками 1 и -1 – это и есть искомые оценочные слова.\n",
    "\n",
    "Алгоритмы распространения метки устроены примерно так: пусть мы находимся в выршине, помеченном +1. С какой-то вероятностью мы переносим эту метку на соседние узлы. С меньшей вероятностью переносим ее на вершины на расстоянии два. В конце распространения метки, часть вершин оказывается помечена меткой +1, часть – -1, большая часть остается без метки.\n",
    "\n",
    "Рекомендуемые алгоритмы распространения метки:\n",
    "1. `graphlab.label_propagation` (`graphlab` доступен бесплатно по образовательной лицензии)\n",
    "2. `sklearn.semi_supervised.LabelPropagation`\n",
    "3. `sklearn.semi_supervised.LabelSpreading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример построения графа \n",
    "\n",
    "import igraph as ig\n",
    "g = ig.Graph(directed=True)\n",
    "for word in model.wv.vocab.keys():\n",
    "    g.add_vertex(word)\n",
    "    \n",
    "    \n",
    "    \n",
    "for word in model.wv.vocab.keys() :\n",
    "    node = g.vs.select(name = word).indices[0]\n",
    "    similar_words = model.most_similar(word, topn=5)\n",
    "    for sim in similar_words:\n",
    "        word1 = sim[0]\n",
    "        val  = sim[1]\n",
    "        new_node = g.vs.select(name = word1).indices[0]\n",
    "        g.add_edge(node, new_node, weight = val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
